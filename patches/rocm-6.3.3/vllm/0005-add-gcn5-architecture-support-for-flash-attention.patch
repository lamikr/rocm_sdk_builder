From 782467381489fb807488cb6f8d553200cdc429a4 Mon Sep 17 00:00:00 2001
From: Mika Laitio <lamikr@pilppa.org>
Date: Sun, 2 Feb 2025 19:32:33 -0800
Subject: [PATCH 5/5] add gcn5 architecture support for flash attention

Signed-off-by: Mika Laitio <lamikr@pilppa.org>
---
 vllm/attention/backends/rocm_flash_attn.py | 6 +++++-
 1 file changed, 5 insertions(+), 1 deletion(-)

diff --git a/vllm/attention/backends/rocm_flash_attn.py b/vllm/attention/backends/rocm_flash_attn.py
index 12110ec7..e0e7ef7f 100644
--- a/vllm/attention/backends/rocm_flash_attn.py
+++ b/vllm/attention/backends/rocm_flash_attn.py
@@ -24,6 +24,10 @@ logger = init_logger(__name__)
 _PARTITION_SIZE_ROCM = 512
 _GPU_ARCH = torch.cuda.get_device_properties("cuda").gcnArchName
 _ON_NAVI = "gfx1" in _GPU_ARCH
+gcn_matches = ["gfx900", "gfx902", "gfx906"]
+_ON_GCN5 = any(gcn_matches in _GPU_ARCH for gcn_matches in _GPU_ARCH)
+print("_ON_GCN5: " + str(_ON_GCN5))
+
 _ON_MI250_MI300 = any(arch in _GPU_ARCH
                       for arch in ["gfx90a", "gfx940", "gfx941", "gfx942"])
 
@@ -883,7 +887,7 @@ def _use_rocm_custom_paged_attention(qtype: torch.dtype, head_size: int,
                                      block_size: int, gqa_ratio: int,
                                      max_seq_len: int) -> bool:
     # rocm custom page attention not support on navi (gfx1*)
-    return (_ON_MI250_MI300 and not _ON_NAVI
+    return (_ON_MI250_MI300 and not _ON_NAVI and not _ON_GCN5
             and (qtype == torch.half or qtype == torch.bfloat16)
             and (head_size == 64 or head_size == 128)
             and (block_size == 16 or block_size == 32)
-- 
2.48.1

