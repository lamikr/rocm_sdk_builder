From 04960d20ed0676caf5936711a6c16fa7e69fdc9b Mon Sep 17 00:00:00 2001
From: Mika Laitio <lamikr@gmail.com>
Date: Sun, 29 Sep 2024 22:13:13 -0700
Subject: [PATCH 4/4] slurm config rocm sdk builder templates

- add 3 slurm.config file templates as an example
  `for different use cases
a) slurmcontroller and 1 work node running both on localhost
   (default)
b) slurmcontroller and 2 work nodes running on same host
c) slurmcontroller and worknode1 running on host rocmsdk1
   and worknode2 running on host rocmsdk2

- following variables needs to be changed by the install part
  of slurm.binfo to the real userid and install target directory
  from these config file templates
     ROCM_SDK_USERID
     ROCM_SDK_INSTALL_PREFIX

Signed-off-by: Mika Laitio <lamikr@gmail.com>
---
 rocm_etc/cgroup.conf                          |   2 +
 ...d_two_nodes_on_different_machines_template | 158 +++++++++++++++++
 ...roller_and_two_nodes_on_localhost_template | 157 +++++++++++++++++
 rocm_etc/slurm.conf_template                  | 160 ++++++++++++++++++
 rocm_etc/slurmdbd.conf                        |  39 +++++
 5 files changed, 516 insertions(+)
 create mode 100644 rocm_etc/cgroup.conf
 create mode 100644 rocm_etc/slurm.conf_example_controller_and_two_nodes_on_different_machines_template
 create mode 100644 rocm_etc/slurm.conf_example_controller_and_two_nodes_on_localhost_template
 create mode 100644 rocm_etc/slurm.conf_template
 create mode 100644 rocm_etc/slurmdbd.conf

diff --git a/rocm_etc/cgroup.conf b/rocm_etc/cgroup.conf
new file mode 100644
index 0000000000..07fb6c2af7
--- /dev/null
+++ b/rocm_etc/cgroup.conf
@@ -0,0 +1,2 @@
+CgroupPlugin=disabled
+ConstrainCores=yes
diff --git a/rocm_etc/slurm.conf_example_controller_and_two_nodes_on_different_machines_template b/rocm_etc/slurm.conf_example_controller_and_two_nodes_on_different_machines_template
new file mode 100644
index 0000000000..ca8b77e712
--- /dev/null
+++ b/rocm_etc/slurm.conf_example_controller_and_two_nodes_on_different_machines_template
@@ -0,0 +1,158 @@
+# slurm.conf file generated by configurator.html.
+# Put this file on all nodes of your cluster.
+# See the slurm.conf man page for more information.
+#
+SlurmctldHost=rocmsdk1
+AuthType=auth/none
+CredType=cred/none
+#SlurmctldHost=
+#
+#DisableRootJobs=NO
+#EnforcePartLimits=NO
+#Epilog=
+#EpilogSlurmctld=
+#FirstJobId=1
+#MaxJobId=999999
+#GresTypes=
+#GroupUpdateForce=0
+#GroupUpdateTime=600
+#JobFileAppend=0
+#JobRequeue=1
+#JobSubmitPlugins=1
+#KillOnBadExit=0
+#LaunchType=launch/slurm
+#Licenses=foo*4,bar
+#MailProg=/bin/mail
+#MaxJobCount=5000
+#MaxStepCount=40000
+#MaxTasksPerNode=128
+MpiDefault=pmix
+#MpiParams=ports=#-#
+#PluginDir=
+#PlugStackConfig=
+#PrivateData=jobs
+ProctrackType=proctrack/linuxproc
+#Prolog=
+#PrologFlags=
+#PrologSlurmctld=
+#PropagatePrioProcess=0
+#PropagateResourceLimits=
+#PropagateResourceLimitsExcept=
+#RebootProgram=
+ReturnToService=1
+#SallocDefaultCommand=
+SlurmctldPidFile=ROCM_SDK_INSTALL_PREFIX/var/run/slurm/slurmctld.pid
+SlurmctldPort=6817
+SlurmdPidFile=ROCM_SDK_INSTALL_PREFIX/var/run/slurm/slurmd_%n.pid
+SlurmdPort=6818
+SlurmdSpoolDir=ROCM_SDK_INSTALL_PREFIX/var/spool/slurmd_%n
+SlurmUser=ROCM_SDK_USERID
+SlurmdUser=ROCM_SDK_USERID
+#SrunEpilog=
+#SrunProlog=
+StateSaveLocation=ROCM_SDK_INSTALL_PREFIX/var/spool/slurmctld
+SwitchType=switch/none
+#TaskEpilog=
+#TaskPlugin=task/affinity,task/cgroup
+TaskPlugin=task/affinity
+TaskPluginParam=Verbose
+#TaskProlog=
+#TopologyPlugin=topology/tree
+#TmpFS=/tmp
+#TrackWCKey=no
+#TreeWidth=
+#UnkillableStepProgram=
+#UsePAM=0
+#
+#
+# TIMERS
+#BatchStartTimeout=10
+#CompleteWait=0
+#EpilogMsgTime=2000
+#GetEnvTimeout=2
+#HealthCheckInterval=0
+#HealthCheckProgram=
+InactiveLimit=0
+KillWait=30
+#MessageTimeout=10
+#ResvOverRun=0
+MinJobAge=300
+#OverTimeLimit=0
+SlurmctldTimeout=120
+SlurmdTimeout=300
+#UnkillableStepTimeout=60
+#VSizeFactor=0
+Waittime=0
+#
+#
+# SCHEDULING
+#DefMemPerCPU=0
+#MaxMemPerCPU=0
+#SchedulerTimeSlice=30
+SchedulerType=sched/backfill
+SelectType=select/cons_tres
+SelectTypeParameters=CR_Core
+#
+#
+# JOB PRIORITY
+#PriorityFlags=
+#PriorityType=priority/basic
+#PriorityDecayHalfLife=
+#PriorityCalcPeriod=
+#PriorityFavorSmall=
+#PriorityMaxAge=
+#PriorityUsageResetPeriod=
+#PriorityWeightAge=
+#PriorityWeightFairshare=
+#PriorityWeightJobSize=
+#PriorityWeightPartition=
+#PriorityWeightQOS=
+#
+#
+# LOGGING AND ACCOUNTING
+#AccountingStorageEnforce=0
+#AccountingStorageHost=
+#AccountingStorageLoc=
+#AccountingStoragePass=
+#AccountingStoragePort=
+AccountingStorageType=accounting_storage/none
+#AccountingStorageUser=
+#AccountingStoreFlag=
+ClusterName=cluster
+#DebugFlags=
+#JobCompHost=
+#JobCompLoc=
+#JobCompPass=
+#JobCompPort=
+JobCompType=jobcomp/none
+#JobCompUser=
+#JobContainerType=job_container/none
+JobAcctGatherFrequency=30
+JobAcctGatherType=jobacct_gather/none
+SlurmctldDebug=info
+#SlurmctldLogFile=
+SlurmdDebug=info
+SlurmdLogFile=ROCM_SDK_INSTALL_PREFIX/var/log/slurm/slurmd_%n.log
+#SlurmSchedLogFile=
+#SlurmSchedLogLevel=
+#
+#
+# POWER SAVE SUPPORT FOR IDLE NODES (optional)
+#SuspendProgram=
+#ResumeProgram=
+#SuspendTimeout=
+#ResumeTimeout=
+#ResumeRate=
+#SuspendExcNodes=
+#SuspendExcParts=
+#SuspendRate=
+#SuspendTime=
+#
+#
+# COMPUTE NODES
+# slurmcontroller runs now on rocmsdk1 instead of localhost
+# 2 nodes run now on different machines, rocmsdk1 and rocmsdk2.
+# port could be even same 6818 for both nodes
+NodeName=rocmsdk1 NodeHostname=rocmsdk1 Port=6818 CPUs=2 State=UNKNOWN
+NodeName=rocmsdk2 NodeHostname=rocmsdk2 Port=6819 CPUs=2 State=UNKNOWN
+PartitionName=debug Nodes=rocmsdk1,rocmsdk2 Default=YES MaxTime=INFINITE State=UP
diff --git a/rocm_etc/slurm.conf_example_controller_and_two_nodes_on_localhost_template b/rocm_etc/slurm.conf_example_controller_and_two_nodes_on_localhost_template
new file mode 100644
index 0000000000..b5f06766cc
--- /dev/null
+++ b/rocm_etc/slurm.conf_example_controller_and_two_nodes_on_localhost_template
@@ -0,0 +1,157 @@
+# slurm.conf file generated by configurator.html.
+# Put this file on all nodes of your cluster.
+# See the slurm.conf man page for more information.
+#
+SlurmctldHost=localhost
+AuthType=auth/none
+CredType=cred/none
+#SlurmctldHost=
+#
+#DisableRootJobs=NO
+#EnforcePartLimits=NO
+#Epilog=
+#EpilogSlurmctld=
+#FirstJobId=1
+#MaxJobId=999999
+#GresTypes=
+#GroupUpdateForce=0
+#GroupUpdateTime=600
+#JobFileAppend=0
+#JobRequeue=1
+#JobSubmitPlugins=1
+#KillOnBadExit=0
+#LaunchType=launch/slurm
+#Licenses=foo*4,bar
+#MailProg=/bin/mail
+#MaxJobCount=5000
+#MaxStepCount=40000
+#MaxTasksPerNode=128
+MpiDefault=pmix
+#MpiParams=ports=#-#
+#PluginDir=
+#PlugStackConfig=
+#PrivateData=jobs
+ProctrackType=proctrack/linuxproc
+#Prolog=
+#PrologFlags=
+#PrologSlurmctld=
+#PropagatePrioProcess=0
+#PropagateResourceLimits=
+#PropagateResourceLimitsExcept=
+#RebootProgram=
+ReturnToService=1
+#SallocDefaultCommand=
+SlurmctldPidFile=ROCM_SDK_INSTALL_PREFIX/var/run/slurm/slurmctld.pid
+SlurmctldPort=6817
+SlurmdPidFile=ROCM_SDK_INSTALL_PREFIX/var/run/slurm/slurmd_%n.pid
+SlurmdPort=6818
+SlurmdSpoolDir=ROCM_SDK_INSTALL_PREFIX/var/spool/slurmd_%n
+SlurmUser=ROCM_SDK_USERID
+SlurmdUser=ROCM_SDK_USERID
+#SrunEpilog=
+#SrunProlog=
+StateSaveLocation=ROCM_SDK_INSTALL_PREFIX/var/spool/slurmctld
+SwitchType=switch/none
+#TaskEpilog=
+#TaskPlugin=task/affinity,task/cgroup
+TaskPlugin=task/affinity
+TaskPluginParam=Verbose
+#TaskProlog=
+#TopologyPlugin=topology/tree
+#TmpFS=/tmp
+#TrackWCKey=no
+#TreeWidth=
+#UnkillableStepProgram=
+#UsePAM=0
+#
+#
+# TIMERS
+#BatchStartTimeout=10
+#CompleteWait=0
+#EpilogMsgTime=2000
+#GetEnvTimeout=2
+#HealthCheckInterval=0
+#HealthCheckProgram=
+InactiveLimit=0
+KillWait=30
+#MessageTimeout=10
+#ResvOverRun=0
+MinJobAge=300
+#OverTimeLimit=0
+SlurmctldTimeout=120
+SlurmdTimeout=300
+#UnkillableStepTimeout=60
+#VSizeFactor=0
+Waittime=0
+#
+#
+# SCHEDULING
+#DefMemPerCPU=0
+#MaxMemPerCPU=0
+#SchedulerTimeSlice=30
+SchedulerType=sched/backfill
+SelectType=select/cons_tres
+SelectTypeParameters=CR_Core
+#
+#
+# JOB PRIORITY
+#PriorityFlags=
+#PriorityType=priority/basic
+#PriorityDecayHalfLife=
+#PriorityCalcPeriod=
+#PriorityFavorSmall=
+#PriorityMaxAge=
+#PriorityUsageResetPeriod=
+#PriorityWeightAge=
+#PriorityWeightFairshare=
+#PriorityWeightJobSize=
+#PriorityWeightPartition=
+#PriorityWeightQOS=
+#
+#
+# LOGGING AND ACCOUNTING
+#AccountingStorageEnforce=0
+#AccountingStorageHost=
+#AccountingStorageLoc=
+#AccountingStoragePass=
+#AccountingStoragePort=
+AccountingStorageType=accounting_storage/none
+#AccountingStorageUser=
+#AccountingStoreFlag=
+ClusterName=cluster
+#DebugFlags=
+#JobCompHost=
+#JobCompLoc=
+#JobCompPass=
+#JobCompPort=
+JobCompType=jobcomp/none
+#JobCompUser=
+#JobContainerType=job_container/none
+JobAcctGatherFrequency=30
+JobAcctGatherType=jobacct_gather/none
+SlurmctldDebug=info
+#SlurmctldLogFile=
+SlurmdDebug=info
+SlurmdLogFile=ROCM_SDK_INSTALL_PREFIX/var/log/slurm/slurmd_%n.log
+#SlurmSchedLogFile=
+#SlurmSchedLogLevel=
+#
+#
+# POWER SAVE SUPPORT FOR IDLE NODES (optional)
+#SuspendProgram=
+#ResumeProgram=
+#SuspendTimeout=
+#ResumeTimeout=
+#ResumeRate=
+#SuspendExcNodes=
+#SuspendExcParts=
+#SuspendRate=
+#SuspendTime=
+#
+#
+# COMPUTE NODES
+NodeName=rocmsdk1 NodeHostname=localhost Port=6818 CPUs=2 State=UNKNOWN
+# If 2 nodenames can not both use the NodeHostname as a parameter (slurm will give error)
+# Workaround is to add localhost_node2 to /etc/hosts file on samle line where localhost is configured for 127.0.0.1 addresss
+NodeName=rocmsdk2 NodeHostname=localhost_node2 Port=6819 CPUs=2 State=UNKNOWN
+PartitionName=debug Nodes=rocmsdk1,rocmsdk2 Default=YES MaxTime=INFINITE State=UP
diff --git a/rocm_etc/slurm.conf_template b/rocm_etc/slurm.conf_template
new file mode 100644
index 0000000000..3549b2b390
--- /dev/null
+++ b/rocm_etc/slurm.conf_template
@@ -0,0 +1,160 @@
+# slurm.conf file generated by configurator.html.
+# Put this file on all nodes of your cluster.
+# See the slurm.conf man page for more information.
+#
+SlurmctldHost=localhost
+AuthType=auth/none
+CredType=cred/none
+#SlurmctldHost=
+#
+#DisableRootJobs=NO
+#EnforcePartLimits=NO
+#Epilog=
+#EpilogSlurmctld=
+#FirstJobId=1
+#MaxJobId=999999
+#GresTypes=
+#GroupUpdateForce=0
+#GroupUpdateTime=600
+#JobFileAppend=0
+#JobRequeue=1
+#JobSubmitPlugins=1
+#KillOnBadExit=0
+#LaunchType=launch/slurm
+#Licenses=foo*4,bar
+#MailProg=/bin/mail
+#MaxJobCount=5000
+#MaxStepCount=40000
+#MaxTasksPerNode=128
+MpiDefault=pmix
+#MpiParams=ports=#-#
+#PluginDir=
+#PlugStackConfig=
+#PrivateData=jobs
+ProctrackType=proctrack/linuxproc
+#Prolog=
+#PrologFlags=
+#PrologSlurmctld=
+#PropagatePrioProcess=0
+#PropagateResourceLimits=
+#PropagateResourceLimitsExcept=
+#RebootProgram=
+ReturnToService=1
+#SallocDefaultCommand=
+SlurmctldPidFile=ROCM_SDK_INSTALL_PREFIX/var/run/slurm/slurmctld.pid
+SlurmctldPort=6817
+SlurmdPidFile=ROCM_SDK_INSTALL_PREFIX/var/run/slurm/slurmd_%n.pid
+SlurmdPort=6818
+SlurmdSpoolDir=ROCM_SDK_INSTALL_PREFIX/var/spool/slurmd_%n
+SlurmUser=ROCM_SDK_USERID
+SlurmdUser=ROCM_SDK_USERID
+#SrunEpilog=
+#SrunProlog=
+StateSaveLocation=ROCM_SDK_INSTALL_PREFIX/var/spool/slurmctld
+SwitchType=switch/none
+#TaskEpilog=
+#TaskPlugin=task/affinity,task/cgroup
+TaskPlugin=task/affinity
+TaskPluginParam=Verbose
+#TaskProlog=
+#TopologyPlugin=topology/tree
+#TmpFS=/tmp
+#TrackWCKey=no
+#TreeWidth=
+#UnkillableStepProgram=
+#UsePAM=0
+#
+#
+# TIMERS
+#BatchStartTimeout=10
+#CompleteWait=0
+#EpilogMsgTime=2000
+#GetEnvTimeout=2
+#HealthCheckInterval=0
+#HealthCheckProgram=
+InactiveLimit=0
+KillWait=30
+#MessageTimeout=10
+#ResvOverRun=0
+MinJobAge=300
+#OverTimeLimit=0
+SlurmctldTimeout=120
+SlurmdTimeout=300
+#UnkillableStepTimeout=60
+#VSizeFactor=0
+Waittime=0
+#
+#
+# SCHEDULING
+#DefMemPerCPU=0
+#MaxMemPerCPU=0
+#SchedulerTimeSlice=30
+SchedulerType=sched/backfill
+SelectType=select/cons_tres
+SelectTypeParameters=CR_Core
+#
+#
+# JOB PRIORITY
+#PriorityFlags=
+#PriorityType=priority/basic
+#PriorityDecayHalfLife=
+#PriorityCalcPeriod=
+#PriorityFavorSmall=
+#PriorityMaxAge=
+#PriorityUsageResetPeriod=
+#PriorityWeightAge=
+#PriorityWeightFairshare=
+#PriorityWeightJobSize=
+#PriorityWeightPartition=
+#PriorityWeightQOS=
+#
+#
+# LOGGING AND ACCOUNTING
+#AccountingStorageEnforce=0
+#AccountingStorageHost=
+#AccountingStorageLoc=
+#AccountingStoragePass=
+#AccountingStoragePort=
+AccountingStorageType=accounting_storage/none
+#AccountingStorageUser=
+#AccountingStoreFlag=
+ClusterName=cluster
+#DebugFlags=
+#JobCompHost=
+#JobCompLoc=
+#JobCompPass=
+#JobCompPort=
+JobCompType=jobcomp/none
+#JobCompUser=
+#JobContainerType=job_container/none
+JobAcctGatherFrequency=30
+JobAcctGatherType=jobacct_gather/none
+SlurmctldDebug=info
+#SlurmctldLogFile=
+SlurmdDebug=info
+SlurmdLogFile=ROCM_SDK_INSTALL_PREFIX/var/log/slurm/slurmd_%n.log
+#SlurmSchedLogFile=
+#SlurmSchedLogLevel=
+#
+#
+# POWER SAVE SUPPORT FOR IDLE NODES (optional)
+#SuspendProgram=
+#ResumeProgram=
+#SuspendTimeout=
+#ResumeTimeout=
+#ResumeRate=
+#SuspendExcNodes=
+#SuspendExcParts=
+#SuspendRate=
+#SuspendTime=
+#
+#
+# COMPUTE NODES
+#NodeName=rocmsdk1 NodeHostname=localhost Port=6818 CPUs=3
+#PartitionName=debug Nodes=rocmsdk1 Default=YES MaxTime=INFINITE State=UP
+
+NodeName=rocmsdk1 NodeHostname=localhost Port=6818 CPUs=2 State=UNKNOWN
+# If 2 nodenames can not both use the NodeHostname as a parameter (slurm will give error)
+# Workaround is to add localhost_node2 to /etc/hosts file on samle line where localhost is configured for 127.0.0.1 addresss
+# NodeName=rocmsdk2 NodeHostname=localhost_node2 Port=6819 CPUs=2 State=UNKNOWN
+PartitionName=debug Nodes=rocmsdk1 Default=YES MaxTime=INFINITE State=UP
diff --git a/rocm_etc/slurmdbd.conf b/rocm_etc/slurmdbd.conf
new file mode 100644
index 0000000000..0e1db76edf
--- /dev/null
+++ b/rocm_etc/slurmdbd.conf
@@ -0,0 +1,39 @@
+#
+# Example slurmdbd.conf file.
+#
+# See the slurmdbd.conf man page for more information.
+#
+# Archive info
+#ArchiveJobs=yes
+#ArchiveDir="/tmp"
+#ArchiveSteps=yes
+#ArchiveScript=
+#JobPurge=12
+#StepPurge=1
+#
+# Authentication info
+AuthType=auth/none
+#AuthInfo=/var/run/munge/munge.socket.2
+#
+# slurmDBD info
+DbdAddr=localhost
+DbdHost=localhost
+#DbdPort=7031
+SlurmUser=slurm
+#MessageTimeout=300
+DebugLevel=verbose
+#DefaultQOS=normal,standby
+LogFile=/opt/rocm_sdk_612/var/log/slurm/slurmdbd.log
+PidFile=/opt/rocm_sdk_612/run/slurm/slurmdbd.pid
+#PluginDir=/usr/lib/slurm
+#PrivateData=accounts,users,usage,jobs
+#TrackWCKey=yes
+#
+# Database info
+StorageType=accounting_storage/mysql
+#StorageHost=localhost
+#StoragePort=1234
+StoragePass=password
+StorageUser=slurm
+#StorageLoc=slurm_acct_db
+
-- 
2.41.1

