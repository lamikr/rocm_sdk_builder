From 615f29cc2b3fa3f6a121e5748e25d861c3dd6b37 Mon Sep 17 00:00:00 2001
From: Arlo Phoenix <aarlo.phoenix@gmail.com>
Date: Sat, 10 Aug 2024 14:53:28 +0200
Subject: [PATCH 4/9] Enable Async Allocator and add hotfix for ROCm 6.2

something in GetWorkspaceSize changed, the changelogs only mentioned that they fixed some
bug, but apparently this changed the logic elsewhere. For now I just keep track of the last workspace size
and reuse that one for the next convolution

Also enabled the async allocator. This made faster-whisper far more consistent and even the top time a bit faster (0.2). It also made whisperX faster (similar absolute speedup, so far higher relative speedup, almost 6%)
---
 README_ROCM.md        | 22 ++++++++++++----------
 src/cuda/allocator.cc |  8 +++++---
 src/ops/conv1d_gpu.cu | 15 ++++++++++-----
 3 files changed, 27 insertions(+), 18 deletions(-)

diff --git a/README_ROCM.md b/README_ROCM.md
index d7b4cfe6..e91be025 100644
--- a/README_ROCM.md
+++ b/README_ROCM.md
@@ -4,7 +4,7 @@
 
 These install instructions are for https://hub.docker.com/r/rocm/pytorch. They should mostly work for system installs as well, but then you'll have to change install directories and make sure all dependencies are installed (in the image they are already present in the conda env)
 
-after following the guide in https://hub.docker.com/r/rocm/pytorch (tested for latest 95ac9ef9b5ec (**ROCm 6.1**))
+after following the guide in https://hub.docker.com/r/rocm/pytorch (tested for latest 9e1748e5b (**ROCm 6.2**))
 
 ```bash
 #init conda
@@ -59,12 +59,12 @@ Other than that I **won't** be adding FA2 or AWQ support. It's written with asse
 
 ## Tested libraries
 
-### faster_whisper
+### faster-whisper
 ```bash
-pip install faster_whisper 
+pip install faster-whisper
 
 #1.0.3 was the most recent version when I made this, so try testing that one first if a newer one doesn't work
-#pip install faster_whisper==1.0.3 
+#pip install faster-whisper==1.0.3
 ```
 
 I included a small benchmark script in this CT2 fork. You need to download a test file from the faster whisper repo
@@ -78,17 +78,19 @@ Then you should be able to run. This per default does just one testrun with the
 python faster_whisper_bench.py
 ```
 
-I'm getting around `11s-12s` on my RX6800 (with model loading included `14s-15s`). 
+I'm getting around `10.9-11.0s` on my RX6800 (with model loading included `13.7-13.8s`).
 
 
-### whisperx
+### whisperX
 
-System dependency is just ffmpeg. Either use your system package mangager or with conda `conda install conda-forge::ffmpeg`
+System dependency is just ffmpeg. Either use your system package manager or with conda `conda install conda-forge::ffmpeg`
 
 ```bash
-pip install whisperx
+pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.1
+pip3 install transformers pandas nltk pyannote.audio==3.1.1 faster_whisper==1.0.1 -U
+pip3 install whisperX --no-deps
 ```
-Python dependencies are a mess here since versions aren't really pinned. I'd recommend just installing faster_whisper from master so you don't run into a bunch of version conflicts. I personally did get it running by pinning numpy==1.23 and then trial and error rolling back stuff till it worked (albeit with a bunch of warnings).
+Python dependencies are a mess here since versions aren't really pinned and the image doesn't come with `torchaudio`. The commands above worked for me though, but will take a while since this reinstalls all python dependencies.
 
 For running you can use its great cli-tool by just using `whisperx path/to/audio` or running my little bench script for the `medium` model.
 
@@ -96,6 +98,6 @@ For running you can use its great cli-tool by just using `whisperx path/to/audio
 python whisperx_bench.py
 ```
 
-this took `4.4s` with language detection and around `4.2s` without.
+this took around `4.1s` with language detection and around `3.94s` without.
 
 If you do get it running it's pretty fast. I excluded model load since that one takes quite a while. With model load it was only slightly faster than faster_whisper, but I think that's connected with the bunch of version conflicts I had. The main advantage of `whisperx` is its great feature set (Forced Alignment, VAD, Speaker Diarization) and the cli-tool (lots of output options), so do try and get it running it's worth it.
\ No newline at end of file
diff --git a/src/cuda/allocator.cc b/src/cuda/allocator.cc
index ddad8627..ad9253df 100644
--- a/src/cuda/allocator.cc
+++ b/src/cuda/allocator.cc
@@ -17,9 +17,11 @@
   #define cudaMallocAsync hipMallocAsync
   #define cudaDeviceGetAttribute hipDeviceGetAttribute 
   #define cudaDevAttrMemoryPoolsSupported hipDeviceAttributeMemoryPoolsSupported
+  #define CT2_USE_ASYNC_ALLOC true
 #else
   #include <cuda.h>
   #include <cub/util_allocator.cuh>
+  #define CT2_USE_ASYNC_ALLOC CUDA_VERSION >= 11020
 #endif
 
 #include <spdlog/spdlog.h>
@@ -76,7 +78,7 @@ namespace ctranslate2 {
     class CudaAsyncAllocator : public Allocator {
     public:
       void* allocate(size_t size, int device_index) override {
-#if CUDA_VERSION >= 11020
+#if CT2_USE_ASYNC_ALLOC
         int prev_device_index = -1;
         if (device_index >= 0) {
           CUDA_CHECK(cudaGetDevice(&prev_device_index));
@@ -99,7 +101,7 @@ namespace ctranslate2 {
       }
 
       void free(void* ptr, int device_index) override {
-#if CUDA_VERSION >= 11020
+#if CT2_USE_ASYNC_ALLOC
         int prev_device_index = -1;
         if (device_index >= 0) {
           CUDA_CHECK(cudaGetDevice(&prev_device_index));
@@ -120,7 +122,7 @@ namespace ctranslate2 {
     };
 
     static bool support_cuda_malloc_async() {
-#if CUDA_VERSION < 11020
+#if !CT2_USE_ASYNC_ALLOC
       return false;
 #else
       for (int i = 0; i < get_gpu_count(); ++i) {
diff --git a/src/ops/conv1d_gpu.cu b/src/ops/conv1d_gpu.cu
index 09637b1c..e7006e42 100644
--- a/src/ops/conv1d_gpu.cu
+++ b/src/ops/conv1d_gpu.cu
@@ -2,6 +2,10 @@
 
 #include "cuda/utils.h"
 
+//needed since apparently GetWorkspaceSize is now has context configuration https://rocm.docs.amd.com/en/latest/about/release-notes.html#miopen-3-2-0
+//and that causes the workspace_size to sometimes not be set which causes significant slowdown because of fallback when finding the algo
+size_t last_workspace_size = 0;
+
 namespace ctranslate2 {
   namespace ops {
 
@@ -58,9 +62,7 @@ namespace ctranslate2 {
 
       miopenHandle_t handle = cuda::get_cudnn_handle();
 
-      miopenConvFwdAlgorithm_t algo = (bias
-                                        ? miopenConvolutionFwdAlgoImplicitGEMM 
-                                        : miopenConvolutionFwdAlgoGEMM);
+      miopenConvFwdAlgorithm_t algo;
 
       size_t workspace_size = 0;
       void* workspace = nullptr;
@@ -70,10 +72,13 @@ namespace ctranslate2 {
                                                           conv_desc,
                                                           output_desc,
                                                           &workspace_size));
-
+      if(workspace_size <= 0)
+        workspace_size = last_workspace_size;
       if (workspace_size > 0)
+      {
         workspace = get_allocator<Device::CUDA>().allocate(workspace_size);
-
+        last_workspace_size = workspace_size;
+      }
       {
       miopenConvAlgoPerf_t convForwardAlgos;
       int algoCount = 1;
-- 
2.47.1

