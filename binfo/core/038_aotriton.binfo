BINFO_APP_NAME=aotriton
BINFO_APP_SRC_SUBDIR_BASENAME=
BINFO_APP_SRC_TOPDIR_BASENAME=${BINFO_APP_NAME}
BINFO_APP_SRC_DIR="${SDK_SRC_ROOT_DIR}/${BINFO_APP_SRC_TOPDIR_BASENAME}"
BINFO_APP_UPSTREAM_REPO_URL=https://github.com/rocmnavi/aotriton.git
#use default git tag
BINFO_APP_UPSTREAM_REPO_VERSION_TAG=rsb_612_aotriton_20241222

func_filter_gfx_array() {
	#local gpu_list_str=$1
	#local -n gpu_accepted_arr="$2"
	local gpu_list_str=$1
	shift
    local default_val=$1
    shift
    local gpu_accepted_arr=("$@")
	local ret

	ret=""
	#echo "gpu_list_str: ${gpu_list_str}"
	for gpu_name in "${gpu_accepted_arr[@]}"; do
        #echo "gpu_name: $gpu_name"
        if [[ ${gpu_list_str} =~ ${gpu_name} ]]; then
            #echo "${gpu_accepted_arr[$ii]}"
            if [ -z ${ret} ]; then
                ret=${gpu_name};
            else
                ret=${ret}\;${gpu_name};
            fi
        fi
    done
    if [ -z ${ret} ]; then
        ret=${default_val};
    fi
    echo "${ret}"
}

# hardcoded list of GPU's where aotriton build works
ACCEPTED_GPU_LIST=(gfx906 gfx908 gfx90a gfx940 gfx941 gfx942 gfx1010 gfx1011 gfx1012 gfx1030 gfx1031 gfx1032 gfx1035 gfx1036 gfx1100 gfx1101 gfx1102 gfx1103 gfx1150 gfx1151)
# take only those GPU's from the array that are in the SEMICOLON_SEPARATED_GPU_TARGET_LIST_DEFAULT
# and if none of the GPU's match, return gfx1100 as a default
FILTERED_GPU_LIST=$(func_filter_gfx_array ${SEMICOLON_SEPARATED_GPU_TARGET_LIST_DEFAULT} gfx1100 "${ACCEPTED_GPU_LIST[@]}")
CFG_TEMP1=-DTARGET_GPUS="${FILTERED_GPU_LIST}"

BINFO_APP_PRE_CONFIG_CMD_ARRAY=(
    "cd ${BINFO_APP_SRC_DIR}"
    "./preconfig_rocm.sh"
)

# aotriton has been patched to check MAX_JOBS environment variable
# in aotriton v2src/CMakeLists.txt and use that for limiting the
# amount of python processes allowed to build and compress hsaco files. 
# This fixes the out of memory problem on cases where computer has lot of
# CPUs compared to amount of memory.
# Note that this fix only works when using Ninja.
# (cmake's limitation for add_custom_jobs command)
export MAX_JOBS=${BUILD_CPU_COUNT_DEFAULT}

BINFO_APP_CMAKE_CFG="-DCMAKE_INSTALL_PREFIX=${INSTALL_DIR_PREFIX_SDK_ROOT}"
BINFO_APP_CMAKE_CFG="${BINFO_APP_CMAKE_CFG} -DCMAKE_PREFIX_PATH=${INSTALL_DIR_PREFIX_SDK_ROOT}/lib64/cmake;${INSTALL_DIR_PREFIX_SDK_ROOT}/lib/cmake"
BINFO_APP_CMAKE_CFG="${BINFO_APP_CMAKE_CFG} ${CFG_TEMP1}"
BINFO_APP_CMAKE_CFG="${BINFO_APP_CMAKE_CFG} -GNinja"
BINFO_APP_CMAKE_CFG="${BINFO_APP_CMAKE_CFG} -DCMAKE_C_COMPILER=${SDK_C_COMPILER_HIPCC}"
BINFO_APP_CMAKE_CFG="${BINFO_APP_CMAKE_CFG} -DCMAKE_CXX_COMPILER=${SDK_CXX_COMPILER_HIPCC}"
BINFO_APP_CMAKE_CFG="${BINFO_APP_CMAKE_CFG} -DROCM_PATH=${INSTALL_DIR_PREFIX_SDK_ROOT}"
BINFO_APP_CMAKE_CFG="${BINFO_APP_CMAKE_CFG} -DROCM_HOME=${INSTALL_DIR_PREFIX_SDK_ROOT}"
BINFO_APP_CMAKE_CFG="${BINFO_APP_CMAKE_CFG} -DTRITON_USE_ROCM=ON"
BINFO_APP_CMAKE_CFG="${BINFO_APP_CMAKE_CFG} -DROCM_DEFAULT_DIR=${INSTALL_DIR_PREFIX_SDK_ROOT}"
BINFO_APP_CMAKE_CFG="${BINFO_APP_CMAKE_CFG} -DAOTRITON_COMPRESS_KERNEL_STATIC_ZSTD=OFF"
BINFO_APP_CMAKE_CFG="${BINFO_APP_CMAKE_CFG} -DMLIR_ENABLE_DUMP=1"
BINFO_APP_CMAKE_CFG="${BINFO_APP_CMAKE_CFG} -DLLVM_IR_ENABLE_DUMP=1"
BINFO_APP_CMAKE_CFG="${BINFO_APP_CMAKE_CFG} -DAMDGCN_ENABLE_DUMP=1"
# causes to build libaotriton.a instead of libaotriton.so (required by pytorch 2.4.0 earlier before we patched to use so)
# BINFO_APP_CMAKE_CFG="${BINFO_APP_CMAKE_CFG} -DAOTRITON_NO_SHARED=1"
# separate build needed to do a backend mode as if this is enabled, other part of build is skipped???
#BINFO_APP_CMAKE_CFG="${BINFO_APP_CMAKE_CFG} -DHIP_BACKEND_MODE=1"
BINFO_APP_CMAKE_CFG="${BINFO_APP_CMAKE_CFG} ${BINFO_APP_SRC_DIR}"

BINFO_APP_BUILD_CMD_ARRAY=(
    "cd ${BINFO_APP_BUILD_DIR}"
    "ninja"
)

BINFO_APP_INSTALL_CMD_ARRAY=(
    "cd ${BINFO_APP_BUILD_DIR}"
    "ninja install"
)

